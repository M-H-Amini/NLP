{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PersianEmbeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPsbR0oTfqM9qMR+C9i7jKO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M-H-Amini/NLP/blob/master/PersianEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJmU5b1S_hlb"
      },
      "source": [
        "#  In The Name Of ALLAH\n",
        "#  Persian Word Embeddings\n",
        "#  Mohammad Hossein Amini (mhamini@aut.ac.ir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFUGb89B_wk0"
      },
      "source": [
        "In this project, I've used a 2-layer neural net to extract word embeddings for persian words. I've used the **CBOW** approach. I make a prediction of the center word from the context with a 2-layer neural net. Weights of each layer after training can be an embedding. So I take average of weights of both layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO8eHPQnMz9d"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from google.colab import drive\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import shutil\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGQr023fNEcR"
      },
      "source": [
        "def createDict(files, min_freq=3):\n",
        "  '''\n",
        "    Creates a dictionary from my corpus.\n",
        "    It returns a dictionary in which for each element,\n",
        "    the key is the word and the value is a tupple in \n",
        "    which the first element is the index and the second\n",
        "    element is the frequency of the word in the corpus.\n",
        "  '''\n",
        "  my_dict = dict()\n",
        "  for file in files:\n",
        "    with open(file) as f:\n",
        "      lines = f.readlines()\n",
        "    for line in lines:\n",
        "      words = line.split()\n",
        "      for word in words:\n",
        "        if word in my_dict.keys():\n",
        "          my_dict[word] += 1\n",
        "        else:\n",
        "          my_dict[word] = 1\n",
        "\n",
        "  final_dict = {key:my_dict[key] for key in my_dict.keys() if my_dict[key]>=min_freq}\n",
        "  final_dict = {key:[c,final_dict[key]] for (c, key) in zip(range(len(final_dict)), final_dict.keys())}\n",
        "  return final_dict\n",
        "\n",
        "files = ['Hafez.txt', 'Saadi.txt']\n",
        "my_dict = createDict(files, 1)\n",
        "dict_len = len(my_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6xWz2YGODNe"
      },
      "source": [
        "print(len(my_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQydYgBKO31Q"
      },
      "source": [
        "def word2onehotvec(word, my_dict):\n",
        "  '''  Gets a word and returs a hot vector corresponding to it  '''\n",
        "  res = np.zeros((dict_len, 1))\n",
        "  res[my_dict[word][0]] = 1\n",
        "  return res\n",
        "\n",
        "print(word2onehotvec('آشفتگان', my_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f1L7m6-Sqkc"
      },
      "source": [
        "def createTrainingset(files, my_dict, N=2):\n",
        "  '''\n",
        "    Create sample of the training set as a list, by words, not vectors.\n",
        "    Each element is a list of 2N + 1 elements. It's first element is the\n",
        "    center word and the others are the context words.\n",
        "  '''\n",
        "  ds = []\n",
        "  for file in files:\n",
        "    with open(file) as f:\n",
        "      lines = f.readlines()\n",
        "    for line in lines:\n",
        "      words = line.split()\n",
        "      for i in range(N, len(words)-N-1):\n",
        "        sample = [words[i]]\n",
        "        sample.extend([words[j] for j in list(range(i-N, i))+list(range(i+1, i+N+1))])\n",
        "        ds.append(sample)\n",
        "  return ds\n",
        "\n",
        "ds = createTrainingset(files, my_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoVnQL2BUrfz"
      },
      "source": [
        "def createVectorizedTrainingset(ds, my_dict):\n",
        "  '''\n",
        "    Turns the dataset from the createTrainingset function\n",
        "    into a vectorized version.\n",
        "  '''\n",
        "  X = []\n",
        "  y = []\n",
        "  dict_len = len(my_dict)\n",
        "  zero = np.zeros_like(word2onehotvec('و', my_dict))\n",
        "  cnt = 0\n",
        "  for sample in ds:\n",
        "    cnt += 1\n",
        "    if cnt > 30000:\n",
        "      break\n",
        "    context_vec = zero.copy()\n",
        "    # print('c', context_vec)\n",
        "    for i in range(len(sample) - 1):\n",
        "      context_vec += word2onehotvec(sample[i+1], my_dict)\n",
        "    context_vec /= (len(sample) - 1)\n",
        "    # print(context_vec, context_vec.dtype)\n",
        "    X.append(context_vec)\n",
        "    y.append(word2onehotvec(sample[0], my_dict))\n",
        "    # input()\n",
        "    # break\n",
        "  print(X[0])\n",
        "  X = np.array(X)[:, :, 0]\n",
        "  y = np.array(y)[:, :, 0]\n",
        "  return X, y\n",
        "    \n",
        "X, y = createVectorizedTrainingset(ds, my_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQNQNhvHWKaf"
      },
      "source": [
        "print(X.shape, y.shape)\n",
        "print(X[0:1])  #  Problem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnwsBqxg6T4w"
      },
      "source": [
        "def saveModel(model, name='persian'):\n",
        "  model.save(name)\n",
        "  shutil.rmtree(os.path.join('gdrive', 'My Drive', name), ignore_errors=True)\n",
        "  shutil.copytree(name, os.path.join('gdrive', 'My Drive', name))\n",
        "  print('Done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxzHT56fo2zT"
      },
      "source": [
        "def loadModel(name):\n",
        "  if os.path.isdir(os.path.join('gdrive', 'My Drive', name)):\n",
        "    shutil.rmtree(name, ignore_errors=True)\n",
        "    shutil.copytree(os.path.join('gdrive', 'My Drive', name), name)\n",
        "    model = keras.models.load_model(name)\n",
        "    print('Model loaded!')\n",
        "    return model\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(keras.layers.Dense(100, activation='sigmoid', input_shape=(len(my_dict),)))\n",
        "  model.add(keras.layers.Dense(len(my_dict), activation='softmax'))\n",
        "  print('Model not found...Created a new one!')\n",
        "  return model\n",
        "\n",
        "model = loadModel('Persian')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXxCQcIwpUQS"
      },
      "source": [
        "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW2R2s9-pabN"
      },
      "source": [
        "model.fit(X, y, batch_size=16, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWL3o0CTxsrH"
      },
      "source": [
        "saveModel(model, 'Persian')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NluHJh1R8min"
      },
      "source": [
        "def extractEmbeddings(model):\n",
        "  '''\n",
        "    Extracts embeddings from the trained model.\n",
        "  '''\n",
        "  ws = model.trainable_weights\n",
        "  w1 = ws[0].numpy()\n",
        "  w2 = ws[2].numpy().T\n",
        "  w3 = (w1 + w2) / 2\n",
        "  return w3\n",
        "\n",
        "embd = extractEmbeddings(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUYlRxGlBhEH"
      },
      "source": [
        "Let's visualize the embeddings in a 2-D plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4Ncy1BU9RTh"
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAFw2Dve9U54"
      },
      "source": [
        "pca = PCA(2)\n",
        "pca.fit(embd)\n",
        "embd2d = pca.transform(embd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4iSqsbC9oxy"
      },
      "source": [
        "chosen_words = ['دوش', 'سحر', 'ملکوت', 'اعلی', 'عشق', 'عاشق', 'معشوق']\n",
        "chosen_indxs = [my_dict[word][0] for word in chosen_words]\n",
        "chosen_2dvecs = embd2d[chosen_indxs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DFvF6SB-ZCV"
      },
      "source": [
        "plt.figure()\n",
        "for i in range(len(chosen_words)):\n",
        "  plt.plot(chosen_2dvecs[i, 0], chosen_2dvecs[i, 1], 'gx')\n",
        "  plt.text(chosen_2dvecs[i, 0], chosen_2dvecs[i, 1], chosen_words[i])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}